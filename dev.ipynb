{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e84361a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "# Access environment variables\n",
    "SECRET_KEY = os.getenv('SERPAPI_SECRET_KEY')\n",
    "\n",
    "if SECRET_KEY is None:\n",
    "    raise ValueError(\"SERPAPI_SECRET_KEY is not set in the environment variables.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b555375",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"artificial intelligence in healthcare\" \n",
    "api = f\"https://serpapi.com/search?engine=google_patents&q={query}&api_key={SECRET_KEY}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1ff1db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(api)\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    patientsdata = data.get('organic_results', [])\n",
    "\n",
    "    with open(\"files/patientsdata.json\", \"w\") as f:\n",
    "        \n",
    "        json.dump(patientsdata, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f2f2625",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_url = f\"https://serpapi.com/search.json?engine=google_patents_details&patent_id=patent%2FCN111127389B%2Fen&api_key={SECRET_KEY}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85d087a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_response = requests.get(patient_url)\n",
    "if patient_response.status_code == 200:\n",
    "    patient_data = patient_response.json()\n",
    "    with open(\"files/patient_detail.json\", \"w\") as f:\n",
    "        json.dump(patient_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "355e6a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The invention provides an extensible artificial intelligence model '\n",
      " 'generation system and method for medical care. Systems and methods for '\n",
      " 'generating artificial intelligence models using synthetic data are '\n",
      " 'disclosed. An example system includes a Deep Neural Network (DNN) generator '\n",
      " 'to generate a first DNN model using first real data. The exemplary system '\n",
      " 'includes a synthetic data generator to generate first synthetic data from '\n",
      " 'the first real data, the first synthetic data to be used by the DNN '\n",
      " 'generator to generate a second DNN model. The exemplary system includes an '\n",
      " 'evaluator to evaluate performance of the first DNN model and the second DNN '\n",
      " 'model to determine whether to generate second composite data. The exemplary '\n",
      " 'system includes a composite data aggregator to aggregate third composite '\n",
      " 'data and fourth composite data from a plurality of sites to form a composite '\n",
      " 'data set. The exemplary system includes an artificial intelligence model '\n",
      " 'deployment processor to deploy an artificial intelligence model trained and '\n",
      " 'tested using the synthetic data set.')\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(patient_data.get('abstract', \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e458135a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('A method for analyzing medical data based on deep learning and an '\n",
      " 'intelligent analyzer thereof can effectively reduce the working pressure of '\n",
      " 'doctors or medical researchers in hospitals, scientifically analyze a large '\n",
      " 'amount of medical or medical data and obtain an analysis result matched with '\n",
      " 'the medical or medical data. The core content of the method is to establish '\n",
      " 'a model in a computer by applying a deep convolution neuron algorithm in '\n",
      " 'deep learning. The model selects and optimizes model parameters by using '\n",
      " 'mass medical data, automatically learns the pathological analysis process of '\n",
      " 'doctors or medical researchers through a training model, then helps the '\n",
      " 'doctors to process mass medical or medical data, and finally assists the '\n",
      " 'doctors to make correct judgment and effective decision aiming at the mass '\n",
      " 'medical data. The invention can greatly reduce the working pressure of '\n",
      " 'doctors or medical researchers, improve the working efficiency of the '\n",
      " 'doctors or medical researchers, and relieve the doctors or medical '\n",
      " 'researchers from the heavy analysis work of medical or medical data, thereby '\n",
      " 'applying more energy to other more important work.')\n",
      "\n",
      "\n",
      "\n",
      "('Present disclose provides the method and system for training and creating '\n",
      " 'neural network model.According to one embodiment, the first set of feature '\n",
      " 'is received, each feature in first set is associated with predetermined '\n",
      " 'classification.To the first set application Bloom filter of feature to '\n",
      " 'generate the second set of feature.Exported by the second set for applying '\n",
      " 'feature to the first node layer of neural network model with generating, to '\n",
      " 'train neural network model, neural network model includes more node layers, '\n",
      " 'and this more node layer intercouples via connection.Compared by the output '\n",
      " 'of neural network model and with the desired value that predetermined '\n",
      " 'classification is associated, to determine whether neural network model '\n",
      " 'meets predetermined condition.')\n",
      "\n",
      "\n",
      "\n",
      "('The natural motion controller constructs a composite motion recognition '\n",
      " 'window by concatenating an adjustable number of consecutive periods of '\n",
      " 'inertial sensor data received from multiple individual sets of inertial '\n",
      " 'sensors. Each of these individual sets of inertial sensors is coupled to a '\n",
      " 'mobile computing device worn, carried, or held by a separate user, or '\n",
      " 'otherwise provides sensor data for these devices. Each synthetic motion '\n",
      " 'recognition window is then passed to a motion recognition model trained by '\n",
      " 'one or more machine-based deep learning processes. The motion recognition '\n",
      " 'model is then applied to the composite motion recognition window to identify '\n",
      " 'one or more predefined motion sequences. The identified movement is then '\n",
      " 'used as a basis for triggering execution of one or more application '\n",
      " 'commands.')\n",
      "\n",
      "\n",
      "\n",
      "('The present invention relates to image-based tumor phenotyping using machine '\n",
      " 'learning from synthetic data. Machine training and application of '\n",
      " 'machine-trained classifiers are used for image-based tumor phenotyping in '\n",
      " 'medical systems. To create a training database with known phenotypic '\n",
      " 'information, synthetic medical images were created. Computational tumor '\n",
      " 'models create various examples of tumors in tissues. Using the computational '\n",
      " 'tumor model allows one to create examples not available from actual '\n",
      " 'patients, thereby increasing the number and variety of examples used for '\n",
      " 'machine learning to predict tumor phenotypes. A model of the imaging system '\n",
      " 'generates a composite image from the examples. The machine-trained '\n",
      " 'classifier is applied to images from an actual patient to predict the '\n",
      " \"patient's tumor phenotype based on the knowledge learned from the synthetic \"\n",
      " 'images.')\n",
      "\n",
      "\n",
      "\n",
      "('Systems and methods are described for predicting depth from colour image '\n",
      " 'data using a statistical model such as a convolutional neural network (CNN), '\n",
      " 'The model is trained on binocular stereo pairs of images, enabling depth '\n",
      " 'data to be predicted from a single source colour image. The model is trained '\n",
      " 'to predict, for each image of an input binocular stereo pair, corresponding '\n",
      " 'disparity values that enable reconstruction of another image when applied, '\n",
      " 'to the image. The model is updated based on a cost function that enforces '\n",
      " 'consistency between the predicted disparity values for each image in the '\n",
      " 'stereo pair.')\n",
      "\n",
      "\n",
      "\n",
      "('What is proposed in the present invention is a kind of based on the twin '\n",
      " 'medical image synthetic method into confrontation network, its main contents '\n",
      " 'includes：Data management, generation confrontation network, train U NET, '\n",
      " 'establish evaluation index and obtain processing picture, its process is, '\n",
      " 'first first stage GAN is managed using DRIVE databases, then first stage GAN '\n",
      " 'generations represent the segmentation masking-out of the variable-geometry '\n",
      " 'of data set, the mask that first stage makes is changed into realistic image '\n",
      " 'by second stage GAN, maker would be classified as real data to minimize its '\n",
      " 'loss function by discriminator, retraining U NET assess the reliability of '\n",
      " 'generated data, finally establish the model that evaluation index weighs '\n",
      " 'generation.Network is resisted by the present invention in that being '\n",
      " 'generated with a pair, a new image constructive ways is created, solves the '\n",
      " 'problems, such as that composograph contains artifact and noise, improve the '\n",
      " 'stability and the sense of reality of image, while become apparent from '\n",
      " 'image detail.')\n",
      "\n",
      "\n",
      "\n",
      "('The invention proposes a weakly supervised image semantic segmentation '\n",
      " 'method based on iteratively mining common features of objects, which belongs '\n",
      " 'to the technical field of pattern recognition. In the training phase, the '\n",
      " 'method obtains a training data set, builds and trains a multi-label '\n",
      " 'classification network, and obtains the initial seed area of the object '\n",
      " 'corresponding to each training image; then, obtains the superpixel area and '\n",
      " 'area label of each training image for training area classification Network, '\n",
      " 'get the updated region label of the superpixel region and use it to train '\n",
      " 'the semantic segmentation network; after iteration, when the performance of '\n",
      " 'the semantic segmentation network converges, the trained semantic '\n",
      " 'segmentation network is obtained; in the use stage, the color image is input '\n",
      " 'into the trained semantic segmentation network , the network outputs the '\n",
      " 'semantic segmentation result of the image. The present invention can realize '\n",
      " 'reliable pixel-level semantic segmentation under the condition of only image '\n",
      " 'category labels, reduce the time and labor cost of data labeling, and has '\n",
      " 'wide application prospects.')\n",
      "\n",
      "\n",
      "\n",
      "('The invention refers to a method and system for parsing a medical image, '\n",
      " 'comprising the steps of:\\n'\n",
      " '- Providing (61) a network, being a multi-task deep neural network\\n'\n",
      " '- Providing (62) the image to be parsed\\n'\n",
      " '- Pre-determining (63) image features derived from the image\\n'\n",
      " '- Defining (64)\\n'\n",
      " '- a main task for image parsing and\\n'\n",
      " '- adjuvant tasks for reconstruction of the image features\\n'\n",
      " '\\n'\n",
      " '- Applying (65) a multi-task learning pipeline to train the network by '\n",
      " 'simultaneously learning the deep neural multi-task network on main and '\n",
      " 'adjuvant tasks in a supervised manner, and to perform multi-tasks '\n",
      " 'prediction, wherein the adjuvant tasks act as regularization of the network '\n",
      " 'during a training phase and are used for deriving an in-build, automatic '\n",
      " 'confidence estimation during a prediction phase.')\n",
      "\n",
      "\n",
      "\n",
      "('A tongue image classification method based on a multi-task convolutional '\n",
      " 'neural network, including a series of preprocessing image operations on the '\n",
      " 'collected tongue images, a deep convolutional neural network for tongue '\n",
      " 'overall feature extraction, and a tongue surface label The region of '\n",
      " 'interest positioning network for detection, the multi-task deep '\n",
      " 'convolutional neural network for deep learning and training and recognition, '\n",
      " 'complete the label classification of tongue color, fur color, and thickness, '\n",
      " 'putrefaction, and moistening properties of tongue. The invention effectively '\n",
      " 'solves the multi-classification problem that the existing methods cannot '\n",
      " 'simultaneously identify multiple attributes such as tongue image, tongue '\n",
      " 'color, fur color, thickness of fur, rancidity, moistening dryness and the '\n",
      " 'like.')\n",
      "\n",
      "\n",
      "\n",
      "('The present invention claims to protect a landmark building recognition and '\n",
      " 'detection method based on deep learning. Predict the binary category of the '\n",
      " 'feature frame and the coordinates of the target building in the original '\n",
      " 'image; then use the RoI Align method to completely map the predicted '\n",
      " 'candidate frame to the feature frame; finally these more accurate feature '\n",
      " 'frames will be classified and bounded. Regression, The predicted '\n",
      " 'probabilities and coordinate positions of different landmark buildings are '\n",
      " 'obtained, the redundant candidate frames are removed by the method of '\n",
      " 'non-maximum value suppression, and the maps with wider area coverage are '\n",
      " 'fused, and finally the identification and detection of landmark buildings '\n",
      " 'are realized. The invention predicts the candidate frame of the landmark '\n",
      " 'building more accurately and has a larger range, and also has better '\n",
      " 'recognition ability for the landmark building image in a complex '\n",
      " 'environment.')\n",
      "\n",
      "\n",
      "\n",
      "('The invention belongs to field of artificial intelligence, disclosing a kind '\n",
      " 'of eye OCT image lesion recognition methods, device and medium, method '\n",
      " 'includes: the original OCT image for obtaining eye, is pre-processed to '\n",
      " 'original OCT image；Pretreated OCT image is inputted in the disaggregated '\n",
      " 'model obtained by training, confidence level vector is obtained by '\n",
      " 'disaggregated model, wherein, disaggregated model includes input layer, '\n",
      " 'convolutional layer, pond layer, full articulamentum and output layer, the '\n",
      " 'input layer is for inputting pretreated OCT image, each layer of input is '\n",
      " 'all from the output of all layers of front in convolutional layer, pond '\n",
      " 'layer and full articulamentum, and it is positive confidence level vector '\n",
      " 'that output layer, which is used to export one or more lesion types,；One or '\n",
      " 'more lesion types of the OCT image are obtained according to the confidence '\n",
      " 'level vector.The present invention only use a disaggregated model can a '\n",
      " \"variety of lesion types in the same portion's OCT image of opening one's \"\n",
      " 'eyes of automatic identification, improving reduces time loss, save the '\n",
      " 'cost.')\n",
      "\n",
      "\n",
      "\n",
      "('The invention discloses a medical endoscope image identification method, a '\n",
      " 'medical endoscope image identification system, medical endoscope image '\n",
      " 'identification equipment and an endoscope image system. The method comprises '\n",
      " 'the following steps: acquiring an original endoscope image according to a '\n",
      " 'medical endoscope video stream; generating a target endoscope image from an '\n",
      " 'original endoscope image obtained by filtering through a neural network; '\n",
      " 'organ information corresponding to the target endoscope image is predicted '\n",
      " 'and identified through classification of a neural network; identifying the '\n",
      " 'image type suitable for the target endoscope image through a classification '\n",
      " 'network according to the corresponding organ information; and under a '\n",
      " 'shooting mode corresponding to the image type, locating a focus area and a '\n",
      " 'focus category according to the part indicated by the organ information in '\n",
      " 'the target endoscope image. The filtering eliminates a large amount of '\n",
      " 'low-quality and noisy images existing in the situations of shooting '\n",
      " 'switching and shaking of the endoscope in the alimentary canal and '\n",
      " 'encountering various liquids and foreign matters, enhances the robustness, '\n",
      " 'realizes classification prediction for the whole process of the endoscope '\n",
      " 'shooting, and realizes systematic and complete image recognition.')\n",
      "\n",
      "\n",
      "\n",
      "('The present invention discloses a kind of medical image means of '\n",
      " 'interpretation, device, computer equipment and storage medium, and this '\n",
      " 'method includes obtaining image analysing computer request, and image '\n",
      " 'analysing computer request includes target medical image；Target medical '\n",
      " 'image is identified using preparatory trained image identification model, '\n",
      " 'obtains the characteristic spectrum of the last layer convolutional layer '\n",
      " 'output in image identification model；Based on characteristic spectrum, the '\n",
      " 'corresponding prediction probability value of each original focus '\n",
      " 'classification of image identification model output is obtained；The original '\n",
      " 'focus classification of maximum predicted probability value is determined as '\n",
      " 'targeted site classification, obtain map weight corresponding with targeted '\n",
      " 'site classification, classification activation mapping is carried out to '\n",
      " 'characteristic spectrum and map weight using activation mapping equation, '\n",
      " 'obtain heating power mapping graph, heating power mapping graph and target '\n",
      " 'medical image are overlapped, target thermodynamic chart is generated, image '\n",
      " 'recognition rate is improved.')\n",
      "\n",
      "\n",
      "\n",
      "('The invention discloses a kind of definite method and device of vehicle '\n",
      " 'attribute structured features, this method includes：Obtain the image '\n",
      " 'sequence of video, image sequence is input to first object detection '\n",
      " 'network, obtain the shallow-layer feature of the first vehicle target and '\n",
      " 'the first vehicle target in every two field picture, the shallow-layer '\n",
      " 'feature of the first vehicle target and first vehicle target in every two '\n",
      " 'field picture, chained list is established for the first vehicle target, '\n",
      " 'determine the first image, the first image is input to multiple-limb target '\n",
      " 'signature identification network is identified, and obtains the structured '\n",
      " 'features of the different attribute of the first vehicle target.It can solve '\n",
      " 'the problems, such as that analyze speed existing for current conventional '\n",
      " 'truck retrieval is slow and recognition effect is poor.')\n",
      "\n",
      "\n",
      "\n",
      "('The invention discloses a method for detecting pulmonary nodules in CT '\n",
      " 'medical images improved by generating an adversarial network. Connected '\n",
      " 'domains composed of binarized images obtain different candidate sets of '\n",
      " 'suspected pulmonary nodules; 4) Establish an auxiliary classifier to '\n",
      " 'generate an adversarial network model to generate positive samples to '\n",
      " 'overcome the imbalance in the number of positive and negative samples; 5) '\n",
      " 'Establish a convolutional neural network The network classifies the '\n",
      " 'suspected pulmonary nodules to obtain the pulmonary nodule area; 6) Use the '\n",
      " 'non-maximum suppression algorithm to obtain the final area of the pulmonary '\n",
      " 'nodule. The invention can make full use of the high-efficiency processing '\n",
      " 'performance of the computer, provide certain scalability, and speed up the '\n",
      " 'data processing efficiency. At the same time, the convolutional neural '\n",
      " 'network algorithm can improve the accuracy of classification, improve the '\n",
      " 'processing performance of CT image data, and construct and analyze more '\n",
      " 'efficiently. Image of lung nodules.')\n",
      "\n",
      "\n",
      "\n",
      "('A method for classifying images of pancreatic cystic tumors based on '\n",
      " 'multi-channel and multi-classifiers, including the following steps: 1) '\n",
      " 'adjusting the window width and window level operation on the original image, '\n",
      " 'Canny edge detection and gradient amplitude calculation to enhance edge '\n",
      " 'features; 2) using ResNet conducts end-to-end training on multi-channel '\n",
      " 'images, uses the output of the pool5 layer as the extracted features, and '\n",
      " 'classifies them with Bayesian classifier and KNN classifier respectively to '\n",
      " 'obtain classification probabilities; Different probabilities are classified '\n",
      " 'to obtain the final result, and the random forest is composed of multiple '\n",
      " 'decision trees. The invention provides a method for classifying pancreatic '\n",
      " 'cystic tumor images based on multi-channel and multi-classifiers, which can '\n",
      " 'automatically perform edge enhancement and improve classification accuracy.')\n",
      "\n",
      "\n",
      "\n",
      "('The present invention provides a kind of lesion report preparing apparatus '\n",
      " 'and methods.Lesion report preparing apparatus includes: computer readable '\n",
      " 'storage medium, for storing the instruction that can be executed by '\n",
      " 'processor；Processor, for executing described instruction to realize '\n",
      " 'following steps: a. obtains medical image；B. the lesion in the medical image '\n",
      " 'is detected, to obtain true lesion；C. malignant and benign lesion is carried '\n",
      " 'out to the true lesion, to obtain the true lesion as pernicious '\n",
      " 'probability；D. signature analysis is carried out to the true lesion, to '\n",
      " 'obtain the feature of the true lesion；And e. is that some or all of '\n",
      " 'pernicious feature of probability and the true lesion generates lesion '\n",
      " 'report according to the true lesion.Lesion report preparing apparatus of the '\n",
      " 'invention and method have also carried out signature analysis to lesion, and '\n",
      " 'can generate the lesion report containing focus characteristic information.')\n",
      "\n",
      "\n",
      "\n",
      "('This disclosure relates to a kind of device and system of the medical image '\n",
      " 'automatic Prediction physiological status from patient, the device includes '\n",
      " 'memory, the computer executable instructions of processor and storage on a '\n",
      " 'memory, wherein, the processor executes following steps when executing the '\n",
      " 'computer executable instructions: detected target object and obtaining '\n",
      " \"corresponding target object image block from institute's received medical \"\n",
      " 'image；The first parameter is determined using the first learning network to '\n",
      " 'each target object image block, first parameter indicates the physiological '\n",
      " 'condition level of respective objects object, and first learning network is '\n",
      " 'trained by adding more than one subsidiary classification layer.The system '\n",
      " 'can utilize learning network quickly, the accurately and automatically '\n",
      " 'prediction target object level and/or image (patient) horizontal '\n",
      " 'physiological status of such as 3D learning network from the medical image '\n",
      " 'of patient.')\n",
      "\n",
      "\n",
      "\n",
      "('The invention discloses a face attribute recognition method based on '\n",
      " 'multi-instance multi-label deep transfer learning, comprising the following '\n",
      " 'steps: preparing a face image data set, and extracting multiple data of a '\n",
      " 'deep convolution neural network transfer model for each face image. A neural '\n",
      " 'layer feature is combined into multi-layer face features; a network model '\n",
      " 'for extracting multi-label relationship features is built, and the '\n",
      " 'multi-layer face features are used as input, and the multi-face attribute '\n",
      " 'labels are true values, and the fixed network model parameters are trained; '\n",
      " 'Design a linear binary classifier for a face attribute, transfer the trained '\n",
      " 'network model of multi-label relationship features as a feature extractor to '\n",
      " 'a multi-face attribute classifier model, and use the face image dataset to '\n",
      " 'train each linear binary classifier . The present invention selects the '\n",
      " 'transfer learning method, transfers the dynamic transfer model to the '\n",
      " 'selected data set quickly and efficiently, builds the multi-label relational '\n",
      " 'feature model with simple training structure, and simultaneously trains the '\n",
      " 'linear binary classification of multiple face attributes. device.')\n",
      "\n",
      "\n",
      "\n",
      "('The embodiment provides a kind of image processing method, device, '\n",
      " 'computer-readable medium and electronic equipments.Described image '\n",
      " 'processing method includes: input medical image to be predicted to model, '\n",
      " 'and the model is multitask depth convolutional neural networks model, and '\n",
      " 'the model includes described image input layer, inclusion layer and n '\n",
      " 'parallel task output layer；Pass through any one or the multiple prediction '\n",
      " 'results for exporting one or more kinds of lesion properties in the n task '\n",
      " 'output layer；Wherein, the model is obtained by the training of n kind '\n",
      " 'medical image training set；N is the positive integer more than or equal to '\n",
      " '2.The technical solution of the embodiment of the present invention carries '\n",
      " 'out the prediction of lesion property by the multitask depth convolutional '\n",
      " 'neural networks model obtained using the training of n kind medical image '\n",
      " 'training set to the medical image of input, so as to improve the accuracy of '\n",
      " 'prediction result.')\n",
      "\n",
      "\n",
      "\n",
      "('The invention discloses a method for recognizing the non-perfusion area of '\n",
      " 'sugar reticulum lesions based on deep learning of fundus angiography images. '\n",
      " 'In the present invention, a multi-layer convolutional neural network is '\n",
      " 'established for three types of fundus angiography images marked by doctors, '\n",
      " 'including non-perfusion area, non-perfusion area and laser spot, and the '\n",
      " 'convolutional neural network is trained based on the fundus angiography '\n",
      " 'image. The final output value of the network conforms to the results marked '\n",
      " 'by the doctor, so that the trained convolutional neural network can be used '\n",
      " 'to automatically detect and identify the non-perfusion area of the fundus. '\n",
      " 'The present invention uses deep learning to automatically learn the required '\n",
      " 'features from the training example fundus angiography image database by '\n",
      " 'training the diagnostically marked fundus angiography images and perform '\n",
      " 'classification and discrimination, and continuously optimizes the data '\n",
      " 'features used for judgment and the convolutional neural network in the '\n",
      " 'training process. Therefore, the sensitivity and specificity of intelligent '\n",
      " 'identification of non-perfusion areas of diabetic retinopathy in clinical '\n",
      " 'application can be greatly improved, and the fundus laser can be accurately '\n",
      " 'assisted.')\n",
      "\n",
      "\n",
      "\n",
      "('An exoskeleton sensory feedback apparatus that senses position and force '\n",
      " 'information from a user and applies forces and torques to a limb. The '\n",
      " 'apparatus has the same number of degrees of freedom as the limb to which it '\n",
      " 'is attached. This is achieved through the use of remote center mechanisms, '\n",
      " 'compliant mechanical design elements and portable self contained electrical '\n",
      " 'motor cooling systems.')\n",
      "\n",
      "\n",
      "\n",
      "('A personal response system which provides speakerphone mode and emergency '\n",
      " 'audio mode by controlling microphone sensitivity when audio signals fall '\n",
      " 'below an acceptable level. Adjustable microphone sensitivity which can be '\n",
      " 'controlled by the response center and/or by the help console itself. '\n",
      " 'Emergency audio mode allows low level voices from outside the room the help '\n",
      " 'console is in to be heard by allowing the response center to control the '\n",
      " 'gradual raising or lowering the microphone sensitivity to vary the listening '\n",
      " 'range of the microphone. A simultaneous increase in speaker volume allows '\n",
      " 'response center personnel to converse with a disabled individual who cannot '\n",
      " 'come to the help console. A second microphone and speaker in the personal '\n",
      " 'transmitter permits two-way voice communication even when the individual is '\n",
      " 'disabled outside of the dwelling and the help console is no longer '\n",
      " 'effective.')\n",
      "\n",
      "\n",
      "\n",
      "('A method for mediating social and behavioral influence processes through an '\n",
      " 'interactive telecommunications guidance system for use in medicine and '\n",
      " 'business (10) that utilizes an expert (200) such as a physician, counselor, '\n",
      " 'manager, supervisor, trainer, or peer in association with a computer (16) '\n",
      " 'that produces and sends a series of motivational messages and/or questions '\n",
      " 'to a client, patient or employee (50) for changing or reinforcing a specific '\n",
      " 'behavioral problem and goal management. The system (10) consists of a client '\n",
      " 'database (12) and a client program (14) that includes for each client unique '\n",
      " 'motivational messages and/or questions based on a model such as the '\n",
      " 'transtheoretical model of change comprising the six stages of behavioral '\n",
      " 'change (100) and the 14 processes of change (114), as interwining, '\n",
      " 'interacting variables in the modification of health, mental health, and work '\n",
      " 'site behaviors of the client or employee (50). The client program (14) in '\n",
      " 'association with the expert (200) utilizes the associated 14 processes of '\n",
      " 'change (114) to move the client (50) through one of the six stages of '\n",
      " 'behavioral change (100) when appropriate by using a plurality of '\n",
      " 'transmission and receiving means. The database and program are operated by a '\n",
      " 'computer (16) that at preselected time periods sends the messages and/or '\n",
      " 'questions to the client (50) through use of a variety of transmission means '\n",
      " 'and furthermore selects a platform of behavioral issues that is to be '\n",
      " 'addressed based on a given behavioral stage or goal (100) at a given time of '\n",
      " 'day.')\n",
      "\n",
      "\n",
      "\n",
      "('The invention provides an extensible artificial intelligence model '\n",
      " 'generation system and method for medical care. Systems and methods for '\n",
      " 'generating artificial intelligence models using synthetic data are '\n",
      " 'disclosed. An example system includes a Deep Neural Network (DNN) generator '\n",
      " 'to generate a first DNN model using first real data. The exemplary system '\n",
      " 'includes a synthetic data generator to generate first synthetic data from '\n",
      " 'the first real data, the first synthetic data to be used by the DNN '\n",
      " 'generator to generate a second DNN model. The exemplary system includes an '\n",
      " 'evaluator to evaluate performance of the first DNN model and the second DNN '\n",
      " 'model to determine whether to generate second composite data. The exemplary '\n",
      " 'system includes a composite data aggregator to aggregate third composite '\n",
      " 'data and fourth composite data from a plurality of sites to form a composite '\n",
      " 'data set. The exemplary system includes an artificial intelligence model '\n",
      " 'deployment processor to deploy an artificial intelligence model trained and '\n",
      " 'tested using the synthetic data set.')\n",
      "\n",
      "\n",
      "\n",
      "('The application discloses a training method of an image segmentation model, '\n",
      " 'an image segmentation method, an image segmentation device and image '\n",
      " 'segmentation equipment, and belongs to the field of image segmentation. The '\n",
      " 'method comprises the following steps: acquiring a sample image, wherein the '\n",
      " 'sample image is an image with an annotation area; performing superpixel '\n",
      " 'division on the sample image to obtain at least two superpixel areas; '\n",
      " 'obtaining a hard label of the pixel according to whether the pixel in the '\n",
      " 'sample image belongs to the labeling area; obtaining a soft label of the '\n",
      " 'pixel according to the super-pixel area to which the pixel belongs and the '\n",
      " 'hard label of the pixel, wherein the soft label is used for representing the '\n",
      " 'pseudo probability that the pixel belongs to the labeling area; and training '\n",
      " 'an image segmentation model according to the hard label of the pixel and the '\n",
      " 'soft label of the pixel. The image segmentation model is trained by using '\n",
      " 'the hard labels and the soft labels of the pixels, so that the trained image '\n",
      " 'segmentation model can accurately segment the segmentation region in the '\n",
      " 'input image, and meanwhile, the training efficiency of the image '\n",
      " 'segmentation model is improved.')\n",
      "\n",
      "\n",
      "\n",
      "('The disclosure provides an image recognition method, device and system and '\n",
      " 'an endoscope image recognition method and device, and relates to the field '\n",
      " 'of artificial intelligence. The method comprises the following steps: '\n",
      " 'acquiring an original image, and inputting the original image into an image '\n",
      " 'recognition model, wherein the image recognition model comprises a network '\n",
      " 'main body structure and a plurality of output layers which are connected '\n",
      " 'with the network main body structure and correspond to different tasks; '\n",
      " 'extracting characteristics of a target object in the original image through '\n",
      " 'the network main structure so as to obtain image characteristics '\n",
      " 'corresponding to the target object; and classifying sub-image features '\n",
      " 'corresponding to the tasks in the image features through the output layers '\n",
      " 'so as to output classification results and characterization information '\n",
      " 'corresponding to the target objects. The method and the device can enable '\n",
      " 'the user to judge the credibility of the classification result obtained by '\n",
      " 'image recognition according to the characterization information and '\n",
      " 'experience, improve the image recognition efficiency and the accuracy of the '\n",
      " 'image recognition result, and further reduce the labor cost.')\n",
      "\n",
      "\n",
      "\n",
      "('Optimizing and/or personalizing activities to a user through artificial '\n",
      " 'intelligence and/or virtual reality.')\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dir_file = os.listdir(\"result\")\n",
    "\n",
    "for file in dir_file:\n",
    "    with open(f\"result/{file}\", \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        pprint.pprint(data.get('abstract', \" \"))\n",
    "        print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9752e32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding length: 768\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pprint import pprint\n",
    "api_embeddings = \"http://localhost:11434/api/embeddings\"\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"model\": \"nomic-embed-text\",\n",
    "    \"prompt\" : 'the quick brown fox jumps over the lazy dog'\n",
    "}\n",
    "\n",
    "response = requests.post(api_embeddings, headers=headers, json=data)\n",
    "\n",
    "response_data = response.json()\n",
    "\n",
    "if response.status_code == 200:\n",
    "    embed = response_data.get('embedding')\n",
    "\n",
    "    print(f\"Embedding length: {len(embed)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67c8359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7292103",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
